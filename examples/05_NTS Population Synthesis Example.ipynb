{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:48:47.722892Z",
     "start_time": "2020-11-10T19:48:46.948691Z"
    }
   },
   "source": [
    "# Converting the National Travel Survey into a Simple MATSim Format Population\n",
    "\n",
    "This notebook demonstrates an example workflow for converting tabular diary data (household attributes, person attributes and trip data) into MATSim formatted xml population data for London households.\n",
    "\n",
    "This includes:\n",
    "- pre-processing of tabular inputs\n",
    "- loading data into pam\n",
    "- household sampling\n",
    "- facility sampling\n",
    "- preliminary investigation\n",
    "- writing to xml\n",
    "\n",
    "This example is highly simplified. Of particular note: the diary data used is spatially very aggregate (trip locations are aggregated to inner/outer London). This creates significant variance in the sampled trip lengths. Generally we would expect more precise spatial data to be used. Alternately the complexity of the facility sampling step can be improved to better account for known trip features such as mode and duration.\n",
    "\n",
    "The diary data used is available from the UK Data Service (https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=5340) and is described here:http://doc.ukdataservice.ac.uk/doc/5340/mrdoc/pdf/5340_nts_user_guidance_1995-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dummy_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:00.355147Z",
     "start_time": "2020-11-23T09:58:59.245038Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:00.383199Z",
     "start_time": "2020-11-23T09:59:00.357541Z"
    }
   },
   "outputs": [],
   "source": [
    "out_dir = '../outputs'  # outputs are writen here\n",
    "\n",
    "# required inputs from the National Travel Survey\n",
    "if use_dummy_data:\n",
    "    households_csv = './data/dummyNTS/householdeul2017.tab'\n",
    "    individuals_csv = './data/dummyNTS/individualeul2017.tab'\n",
    "    trips_csv ='./data/dummyNTS/tripeul2017.tab'\n",
    "\n",
    "else:\n",
    "    households_csv = '~/Data/UKDA-5340-tab/tab/householdeul2017.tab'\n",
    "    individuals_csv = '~/Data/UKDA-5340-tab/tab/individualeul2017.tab'\n",
    "    trips_csv ='~/Data/UKDA-5340-tab/tab/tripeul2017.tab'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T17:13:00.123065Z",
     "start_time": "2020-08-03T17:13:00.093023Z"
    }
   },
   "source": [
    "## Load households data\n",
    "\n",
    "1. Load household data into pandas DataFrame.\n",
    "2. Create some mappings of participation and weighting by household for use later. These are described in http://doc.ukdataservice.ac.uk/doc/5340/mrdoc/pdf/5340_nts_user_guidance_1995-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:02.359648Z",
     "start_time": "2020-11-23T09:59:01.049612Z"
    }
   },
   "outputs": [],
   "source": [
    "hh_in = pd.read_csv(\n",
    "    households_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['HouseholdID', 'SurveyYear', 'PSUID', 'W2', 'OutCom_B02ID',\n",
    "       'HHIncome2002_B02ID', 'AddressType_B01ID', 'Ten1_B02ID',\n",
    "       'Landlord_B01ID', 'ResLength_B01ID', 'HHoldCountry_B01ID',\n",
    "       'HHoldGOR_B02ID', 'HHoldNumAdults', 'HHoldNumChildren',\n",
    "       'HHoldNumPeople', 'HHoldStruct_B02ID', 'NumLicHolders',\n",
    "       'HHoldEmploy_B01ID', 'NumVehicles', 'NumBike', 'NumCar', 'NumMCycle',\n",
    "       'NumVanLorry', 'NumCarVan', 'WalkBus_B01ID', 'Getbus_B01ID',\n",
    "       'WalkRail_B01ID', 'WalkRailAlt_B01ID',\n",
    "       'HRPWorkStat_B02ID', 'HRPSEGWorkStat_B01ID', 'HHoldOAClass2011_B03ID',\n",
    "       'Settlement2011EW_B03ID', 'Settlement2011EW_B04ID'],\n",
    ")\n",
    "\n",
    "hh_in.HHIncome2002_B02ID = pd.to_numeric(hh_in.HHIncome2002_B02ID, errors='coerce')\n",
    "hh_in.NumLicHolders = pd.to_numeric(hh_in.NumLicHolders, errors='coerce')\n",
    "hh_in.NumVehicles = pd.to_numeric(hh_in.NumVehicles, errors='coerce')\n",
    "hh_in.NumCar = pd.to_numeric(hh_in.NumCar, errors='coerce')\n",
    "hh_in.NumMCycle = pd.to_numeric(hh_in.NumMCycle, errors='coerce')\n",
    "hh_in.NumVanLorry = pd.to_numeric(hh_in.NumVanLorry, errors='coerce')\n",
    "hh_in.NumCarVan = pd.to_numeric(hh_in.NumCarVan, errors='coerce')\n",
    "hh_in.Settlement2011EW_B04ID = pd.to_numeric(hh_in.Settlement2011EW_B04ID, errors='coerce')\n",
    "\n",
    "hh_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:02.471138Z",
     "start_time": "2020-11-23T09:59:02.362042Z"
    }
   },
   "outputs": [],
   "source": [
    "participation_mapping = dict(zip(hh_in.HouseholdID, hh_in.OutCom_B02ID))\n",
    "weight_mapping = dict(zip(hh_in.HouseholdID, hh_in.W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T10:49:47.862682Z",
     "start_time": "2020-08-04T10:49:47.799479Z"
    }
   },
   "source": [
    "## Load person data\n",
    "\n",
    "Load person attributes data into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:06.030164Z",
     "start_time": "2020-11-23T09:59:02.474033Z"
    }
   },
   "outputs": [],
   "source": [
    "persons_in = pd.read_csv(\n",
    "    individuals_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['SurveyYear', 'IndividualID', 'HouseholdID', 'PSUID', 'VehicleID',\n",
    "       'PersNo', 'Age_B01ID', 'OfPenAge_B01ID', 'Sex_B01ID', 'EdAttn1_B01ID',\n",
    "       'EdAttn2_B01ID', 'EdAttn3_B01ID', 'DrivLic_B02ID', 'CarAccess_B01ID',\n",
    "       'DrivDisable_B01ID', 'WkPlace_B01ID', 'ES2000_B01ID', 'NSSec_B03ID',\n",
    "       'SC_B01ID', 'Stat_B01ID', 'SVise_B01ID', 'EcoStat_B02ID',\n",
    "       'PossHom_B01ID']\n",
    ")\n",
    "persons_in.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trip data\n",
    "\n",
    "1. Load trip data into pandas DataFrame format.\n",
    "2. Apply some preliminary formatting\n",
    "3. Replace headers so that we can use pam read method:\n",
    "\n",
    "\n",
    "- pid - person ID\n",
    "- hid - household ID\n",
    "- seq - trip sequence number\n",
    "- hzone - household zone\n",
    "- ozone - trip origin zone\n",
    "- dzone - trip destination zone\n",
    "- purp - trip purpose\n",
    "- mode - trip mode\n",
    "- tst - trip start time (minutes)\n",
    "- tet - trip end time (minutes)\n",
    "- freq - weighting for representative population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:19.495376Z",
     "start_time": "2020-11-23T09:59:06.038395Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries_in = pd.read_csv(\n",
    "    trips_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['TripID', 'SurveyYear', 'DayID', 'IndividualID', 'HouseholdID', 'PSUID',\n",
    "       'PersNo', 'TravDay', 'JourSeq', 'ShortWalkTrip_B01ID', 'NumStages',\n",
    "       'MainMode_B04ID', 'TripPurpFrom_B01ID',\n",
    "        'TripPurpTo_B01ID', 'TripPurpose_B04ID',\n",
    "       'TripStart', 'TripEnd', 'TripOrigUA2009_B01ID', 'TripDestUA2009_B01ID'],\n",
    "#     dtype={\"W5\": np.float64,}\n",
    ")\n",
    "\n",
    "travel_diaries_in.TripStart = pd.to_numeric(travel_diaries_in.TripStart, errors='coerce')\n",
    "travel_diaries_in.TripEnd = pd.to_numeric(travel_diaries_in.TripEnd, errors='coerce')\n",
    "\n",
    "travel_diaries_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:19.921306Z",
     "start_time": "2020-11-23T09:59:19.500969Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries_in['participation'] = travel_diaries_in.HouseholdID.map(participation_mapping)\n",
    "travel_diaries_in['hh_weight'] = travel_diaries_in.HouseholdID.map(weight_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.535710Z",
     "start_time": "2020-11-23T09:59:19.924030Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries = travel_diaries_in.loc[travel_diaries_in.participation.isin([1,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.576839Z",
     "start_time": "2020-11-23T09:59:21.538108Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.676906Z",
     "start_time": "2020-11-23T09:59:21.579327Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.rename(\n",
    "    columns={  # rename data\n",
    "        'JourSeq': 'seq',\n",
    "        'TripOrigUA2009_B01ID': 'ozone',\n",
    "        'TripDestUA2009_B01ID': 'dzone',\n",
    "        'TripPurpFrom_B01ID': 'oact',\n",
    "        'TripPurpTo_B01ID': 'dact',\n",
    "        'MainMode_B04ID': 'mode',\n",
    "        'TripStart': 'tst',\n",
    "        'TripEnd': 'tet',\n",
    "    },\n",
    "                inplace=True)\n",
    "\n",
    "travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.720775Z",
     "start_time": "2020-11-23T09:59:21.680364Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.767674Z",
     "start_time": "2020-11-23T09:59:21.729518Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_uniques(df):\n",
    "    for c in df.columns:\n",
    "        print(c)\n",
    "        n = df[c].nunique()\n",
    "        if n < 1000:\n",
    "            print(df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.922539Z",
     "start_time": "2020-11-23T09:59:21.772210Z"
    }
   },
   "outputs": [],
   "source": [
    "check_uniques(travel_diaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Mapping\n",
    "\n",
    "The NTS documentation refers to a 'modified' 2009 Unitary Authorities. The Unmodified 2017 UAs are included below for reference. They 2017 UA names are similar but not the same as the NTS mappings.\n",
    "\n",
    "Found here: https://data.gov.uk/dataset/4e1d5b2c-bb91-42ad-b420-f7fcab638389/counties-and-unitary-authorities-december-2017-full-extent-boundaries-in-uk-wgs84.\n",
    "\n",
    "We have built our own geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.952200Z",
     "start_time": "2020-11-23T09:59:21.925277Z"
    }
   },
   "outputs": [],
   "source": [
    "area_path = \"./data/dummyNTS/NTSareas.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:32.378392Z",
     "start_time": "2020-11-23T09:59:21.955206Z"
    }
   },
   "outputs": [],
   "source": [
    "areas = gp.read_file(area_path)\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:33.886738Z",
     "start_time": "2020-11-23T09:59:32.383388Z"
    }
   },
   "outputs": [],
   "source": [
    "areas.plot(figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean out incomplete plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:33.918312Z",
     "start_time": "2020-11-23T09:59:33.888534Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_broken_plans(plan):\n",
    "    if plan.isnull().values.any():\n",
    "        return None\n",
    "    for col in ['ozone', 'dzone']:\n",
    "        if -8 in list(plan[col]):\n",
    "            return None\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:57.830263Z",
     "start_time": "2020-11-23T09:59:33.923065Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_travel_diaries = travel_diaries.groupby(\n",
    "    ['IndividualID', 'TravDay']\n",
    ").apply(\n",
    "    remove_broken_plans\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:57.970972Z",
     "start_time": "2020-11-23T10:04:57.833810Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:58.083788Z",
     "start_time": "2020-11-23T10:04:57.992403Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(travel_diaries))\n",
    "print(len(clean_travel_diaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Mappings and apply to common fields\n",
    "\n",
    "We simplify key trip variables such as mode and activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:58.172340Z",
     "start_time": "2020-11-23T10:04:58.103360Z"
    }
   },
   "outputs": [],
   "source": [
    "def string_to_dict(string):\n",
    "    \"\"\"used to build dicts from NTS rtf format dictionaries (cut and paste from the NTS documentation)\"\"\"\n",
    "    mapping = {}\n",
    "    for line in string.split(\"\\n\"):\n",
    "        _, v, l = line.split(\"\\t\")\n",
    "        v = v.split(\" = \")[1]\n",
    "        l = l.split(\" = \")[1]\n",
    "        mapping[float(v)] = str(l)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:58.466019Z",
     "start_time": "2020-11-23T10:04:58.185590Z"
    }
   },
   "outputs": [],
   "source": [
    "mode_mapping = {\n",
    "    1: 'walk',\n",
    "     2: 'bike',\n",
    "     3: 'car',  #'Car/van driver'\n",
    "     4: 'car',  #'Car/van driver'\n",
    "     5: 'car',  #'Motorcycle',\n",
    "     6: 'car',  #'Other private transport',\n",
    "     7: 'pt', #Bus in London',\n",
    "     8: 'pt', #'Other local bus',\n",
    "     9: 'pt', #'Non-local bus',\n",
    "     10: 'pt', #'London Underground',\n",
    "     11: 'pt', #'Surface Rail',\n",
    "     12: 'car',  #'Taxi/minicab',\n",
    "     13: 'pt', #'Other public transport',\n",
    "     -10: 'DEAD',\n",
    "     -8: 'NA'\n",
    "}\n",
    "\n",
    "purp_mapping = {\n",
    "    1: 'work',\n",
    "     2: 'work',  #'In course of work',\n",
    "     3: 'education',\n",
    "     4: 'shop',  #'Food shopping',\n",
    "     5: 'shop',  #'Non food shopping',\n",
    "     6: 'medical', #'Personal business medical',\n",
    "     7: 'other',  #'Personal business eat/drink',\n",
    "     8: 'other',  #'Personal business other',\n",
    "     9: 'other',  #'Eat/drink with friends',\n",
    "     10: 'visit',  #'Visit friends',\n",
    "     11: 'other',  #'Other social',\n",
    "     12: 'other',  #'Entertain/ public activity',\n",
    "     13: 'other',  #'Sport: participate',\n",
    "     14: 'home',  #'Holiday: base',\n",
    "     15: 'other',  #'Day trip/just walk',\n",
    "     16: 'other',  #'Other non-escort',\n",
    "     17: 'escort',  #'Escort home',\n",
    "     18: 'escort',  #'Escort work',\n",
    "     19: 'escort',  #'Escort in course of work',\n",
    "     20: 'escort',  #'Escort education',\n",
    "     21: 'escort',  #'Escort shopping/personal business',\n",
    "     22: 'escort',  #'Other escort',\n",
    "     23: 'home',  #'Home',\n",
    "     -10: 'DEAD',\n",
    "     -8: 'NA'\n",
    "}\n",
    "\n",
    "clean_travel_diaries['mode'] = clean_travel_diaries['mode'].map(mode_mapping)\n",
    "clean_travel_diaries['oact'] = clean_travel_diaries['oact'].map(purp_mapping)\n",
    "clean_travel_diaries['dact'] = clean_travel_diaries['dact'].map(purp_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweight and Split Days\n",
    "\n",
    "In order to get the most from our small sample we treat individual diary days as new persons. In order to maintain the original household weighting we reduce this accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:07:34.500482Z",
     "start_time": "2020-11-23T10:04:58.468222Z"
    }
   },
   "outputs": [],
   "source": [
    "# reweight and split ids for unique days\n",
    "\n",
    "def reweight(group):\n",
    "    \"\"\"\n",
    "    Reweight based on multiple diary days, ie if an agent has two diary days, we will treat these as\n",
    "    two unique agents, so we half the original weighting\n",
    "    \"\"\"\n",
    "    group['freq'] = group.hh_weight / group.DayID.nunique()\n",
    "    return group\n",
    "\n",
    "trips = clean_travel_diaries.groupby('IndividualID').apply(reweight)\n",
    "trips['pid'] = [f\"{p}_{d}\" for p, d in zip(trips.IndividualID, trips.TravDay)]\n",
    "trips['hid'] = [f\"{h}_{d}\" for h, d in zip(trips.HouseholdID, trips.TravDay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:07:34.642431Z",
     "start_time": "2020-11-23T10:07:34.516529Z"
    }
   },
   "outputs": [],
   "source": [
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:07:34.763851Z",
     "start_time": "2020-11-23T10:07:34.712210Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_days(\n",
    "    trips,\n",
    "    target,\n",
    "    trips_on='Diary_number',\n",
    "    target_on='Diary_number',\n",
    "    new_id='pid',\n",
    "    trim=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Expand target df based on mapping between trips target_on and new_id.\n",
    "    This is so slow. Fix\n",
    "    Set index to new_id.\n",
    "    \"\"\"\n",
    "    print(\"Building mapping.\")\n",
    "    mapping = {}\n",
    "    for i, person in trips.groupby(target_on):\n",
    "        mapping[i] = list(set(person[new_id]))\n",
    "    n = len(mapping)\n",
    "    \n",
    "    if trim:\n",
    "        print(\"Trimming target.\")\n",
    "        selection = set(trips[trips_on])\n",
    "        target = target.loc[target[target_on].isin(selection)]\n",
    "    \n",
    "    expanded = pd.DataFrame()\n",
    "    for p, (i, ids) in enumerate(mapping.items()):\n",
    "        if not p % 10:\n",
    "            print(f\"Building expanded data {p}/{n}\", end='\\r', flush=True)\n",
    "        for idx in ids:\n",
    "            split = target.loc[target[target_on] == i]\n",
    "            split[new_id] = idx\n",
    "            expanded = expanded.append(split)\n",
    "    expanded.set_index(new_id, inplace=True)\n",
    "    print(f\"Done\")\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:58:31.895656Z",
     "start_time": "2020-11-23T10:07:34.767754Z"
    }
   },
   "outputs": [],
   "source": [
    "hhs = expand_days(\n",
    "    trips,\n",
    "    hh_in,\n",
    "    trips_on='HouseholdID',\n",
    "    target_on='HouseholdID',\n",
    "    new_id='hid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:58:31.978135Z",
     "start_time": "2020-11-23T10:58:31.915713Z"
    }
   },
   "outputs": [],
   "source": [
    "hhs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:07.759760Z",
     "start_time": "2020-11-23T10:58:31.993216Z"
    }
   },
   "outputs": [],
   "source": [
    "people = expand_days(\n",
    "    trips,\n",
    "    persons_in,\n",
    "    trips_on='IndividualID',\n",
    "    target_on='IndividualID',\n",
    "    new_id='pid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:07.823487Z",
     "start_time": "2020-11-23T12:10:07.762160Z"
    }
   },
   "outputs": [],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into PAM\n",
    "\n",
    "We load the pandas formatted data into Pam using the `pam.read.load_travel_diary_from_to` read method. We do some very preliminary validation of plans and assurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:07.959695Z",
     "start_time": "2020-11-23T12:10:07.835753Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam import write\n",
    "from pam import read\n",
    "from pam.plot.stats import plot_activity_times, plot_leg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:08.008552Z",
     "start_time": "2020-11-23T12:10:07.963560Z"
    }
   },
   "outputs": [],
   "source": [
    "trips.tst = trips.tst.astype(int)\n",
    "trips.tet = trips.tet.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:17.801118Z",
     "start_time": "2020-11-23T12:10:08.014383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population = read.load_travel_diary(\n",
    "    trips=trips,\n",
    "    persons_attributes=people,\n",
    "    hhs_attributes=hhs,\n",
    "    trip_freq_as_person_freq=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:17.999106Z",
     "start_time": "2020-11-23T12:10:17.804047Z"
    }
   },
   "outputs": [],
   "source": [
    "population.fix_plans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.064982Z",
     "start_time": "2020-11-23T12:10:18.001901Z"
    }
   },
   "outputs": [],
   "source": [
    "# this should be replaced with a more direct method\n",
    "for hh in population.households.values():\n",
    "    for p in hh.people.values():\n",
    "        p.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.106291Z",
     "start_time": "2020-11-23T12:10:18.067138Z"
    }
   },
   "outputs": [],
   "source": [
    "population.size  # this also accounts for the weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.169445Z",
     "start_time": "2020-11-23T12:10:18.119285Z"
    }
   },
   "outputs": [],
   "source": [
    "population.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.219002Z",
     "start_time": "2020-11-23T12:10:18.176008Z"
    }
   },
   "outputs": [],
   "source": [
    "population.activity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.270601Z",
     "start_time": "2020-11-23T12:10:18.222929Z"
    }
   },
   "outputs": [],
   "source": [
    "population.mode_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:20.473655Z",
     "start_time": "2020-11-23T12:10:18.277070Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_activity_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:21.682828Z",
     "start_time": "2020-11-23T12:10:20.483760Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_leg_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:21.728035Z",
     "start_time": "2020-11-23T12:10:21.693486Z"
    }
   },
   "outputs": [],
   "source": [
    "# night shift @ 2016008863_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T10:58:14.515344Z",
     "start_time": "2020-11-25T10:58:14.237045Z"
    }
   },
   "outputs": [],
   "source": [
    "hh = population.random_household()\n",
    "hh.print()\n",
    "hh.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T10:58:58.742366Z",
     "start_time": "2020-11-25T10:58:58.698118Z"
    }
   },
   "outputs": [],
   "source": [
    "population.activity_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Population\n",
    "\n",
    "We sample a very small population based on the given NTS household weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:26.313000Z",
     "start_time": "2020-11-23T12:10:22.120024Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam.core import Population\n",
    "from pam.samplers.basic import freq_sample\n",
    "from copy import deepcopy\n",
    "\n",
    "population_sample = Population()\n",
    "    \n",
    "for hid, household in population.households.items():\n",
    "    av_hh_weight = household.freq  # this is currently the av of person freq in the hh\n",
    "    freq = freq_sample(av_hh_weight, 10)\n",
    "\n",
    "    for idx in range(freq):\n",
    "        hh = deepcopy(household)\n",
    "        hh.hid = f\"{hh.hid}_{idx}\"\n",
    "        hh.people = {}\n",
    "        for pid, person in household.people.items():\n",
    "            p = deepcopy(person)\n",
    "            p.pid = f\"{pid}_{idx}\"\n",
    "            hh.add(p)\n",
    "        population_sample.add(hh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:26.354636Z",
     "start_time": "2020-11-23T12:10:26.315503Z"
    }
   },
   "outputs": [],
   "source": [
    "population_sample.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facility Sampling¶ \n",
    "\n",
    "The facilities input is prepared using a separate project called OSM-Facility Sampler (OSMFS). This project woulbe be better names the OSM Facility *Extractor*. We use it to extract viable activity locations for each activity type for each zone. This project is not currently open source, but is described below:\n",
    "\n",
    "OSMFS joins osm data with the geographies of an area to create a mapping between zones, acts and facility locations (points). This is output as a geojson:\n",
    "\n",
    "{\"type\": \"FeatureCollection\", \"features\": [{\"id\": \"0\", \"type\": \"Feature\", \"properties\": {\"activity\": \"other\"}, \"geometry\": {\"type\": \"Point\", \"coordinates\": [-4.5235751, 54.1698685]}},\n",
    "\n",
    "todo: the current methodology does not support shared facilities, ie facilities with more than one activity (schools are places of education and work for example).\n",
    "\n",
    "todo: the above json has to be rejoined with the geography to create a spatial sampler. This is a duplicated operation which could be included in the Bench output, eg:\n",
    "\n",
    "zone_id: activity: (id, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T19:41:13.503578Z",
     "start_time": "2020-11-23T19:41:13.475775Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam.samplers import facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_facilities(path, from_crs=\"EPSG:4326\", to_crs=\"EPSG:27700\"):\n",
    "    \n",
    "    facilities = gp.read_file(facilities_path)\n",
    "    facilities.crs = from_crs\n",
    "    facilities.to_crs(to_crs, inplace=True)\n",
    "    return facilities\n",
    "\n",
    "def load_zones(zones_path, from_crs=\"EPSG:27700\", to_crs=\"EPSG:27700\"):\n",
    "    \n",
    "    zones = gp.read_file(zones_path)\n",
    "    zones.set_index('id', inplace=True)\n",
    "    if not from_crs == to_crs:\n",
    "        zones.crs = from_crs\n",
    "        zones.to_crs(to_crs, inplace=True)\n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T19:41:13.882483Z",
     "start_time": "2020-11-23T19:41:13.854988Z"
    }
   },
   "outputs": [],
   "source": [
    "# facilities_path = '~Data/facilities/NTS_london_facilities.json'\n",
    "\n",
    "# facilities = load_facilities(facilities_path)\n",
    "# zones = load_zones(area_path, from_crs=\"EPSG:4326\")\n",
    "\n",
    "# facility_sampler = facility.FacilitySampler(\n",
    "# facilities=facilities,\n",
    "# zones=zones,\n",
    "# build_xml=True,\n",
    "# fail=False,\n",
    "# random_default=True\n",
    "# )\n",
    "\n",
    "# facility_sampler.clear()\n",
    "# population_sample.sample_locs(facility_sampler)"
   ]
  },
  {
   "source": [
    "# Random Sampler\n",
    "\n",
    "Failing a facility sampler - we can use random sampling instead."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pam.samplers.spatial import RandomPointSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = load_zones(area_path, from_crs=\"EPSG:4326\")\n",
    "sampler = RandomPointSampler(geoms=zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.sample(530, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sample.sample_locs(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:51:18.843528Z",
     "start_time": "2020-11-24T18:51:18.545189Z"
    }
   },
   "outputs": [],
   "source": [
    "person = population_sample.random_person()\n",
    "person.plot()\n",
    "person.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:48:33.844210Z",
     "start_time": "2020-11-11T11:48:33.815280Z"
    }
   },
   "source": [
    "## Write to Disk\n",
    "\n",
    "1. write MATSim formats to disk (plans and attributes)\n",
    "2. write csv and geojson summaries to disk\n",
    "3. write MATSim formatted facilities to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pam.write as write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:52:43.705393Z",
     "start_time": "2020-11-24T18:52:28.634010Z"
    }
   },
   "outputs": [],
   "source": [
    "comment = 'NTS london prelim 24nov2020 epsg27700'\n",
    "\n",
    "write.write_matsim(\n",
    "        population_sample,\n",
    "        plans_path=os.path.join(out_dir, 'plans.xml'),\n",
    "        attributes_path=os.path.join(out_dir, 'attributes.xml'),\n",
    "        comment=comment\n",
    "    )\n",
    "population_sample.to_csv(out_dir, crs=\"EPSG:27700\", to_crs=\"EPSG:4326\")\n",
    "# facility_sampler.write_facilities_xml(os.path.join(out_dir, 'facilities.xml'), comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}