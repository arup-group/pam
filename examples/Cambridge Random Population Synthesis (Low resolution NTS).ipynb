{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:48:47.722892Z",
     "start_time": "2020-11-10T19:48:46.948691Z"
    }
   },
   "source": [
    "# Converting the National Travel Survey into a Simple MATSim Format Population\n",
    "\n",
    "This notebook demonstrates an example workflow for converting tabular diary data (household attributes, person attributes and trip data) into MATSim formatted xml population data for London households.\n",
    "\n",
    "This includes:\n",
    "- pre-processing of tabular inputs\n",
    "- loading data into pam\n",
    "- household sampling\n",
    "- facility sampling\n",
    "- preliminary investigation\n",
    "- writing to xml\n",
    "\n",
    "This example is highly simplified. Of particular note: the diary data used is spatially very aggregate (trip locations are aggregated to inner/outer London). This creates significant variance in the sampled trip lengths. Generally we would expect more precise spatial data to be used. Alternately the complexity of the facility sampling step can be improved to better account for known trip features such as mode and duration.\n",
    "\n",
    "The diary data used is available from the UK Data Service (https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=5340) and is described here:http://doc.ukdataservice.ac.uk/doc/5340/mrdoc/pdf/5340_nts_user_guidance_1995-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dummy_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:00.355147Z",
     "start_time": "2020-11-23T09:58:59.245038Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:00.383199Z",
     "start_time": "2020-11-23T09:59:00.357541Z"
    }
   },
   "outputs": [],
   "source": [
    "out_dir = '../outputs'  # outputs are writen here\n",
    "\n",
    "# required inputs from the National Travel Survey\n",
    "if use_dummy_data:\n",
    "    households_csv = './data/dummyNTS/householdeul2017.tab'\n",
    "    individuals_csv = './data/dummyNTS/individualeul2017.tab'\n",
    "    trips_csv ='./data/dummyNTS/tripeul2017.tab'\n",
    "\n",
    "else:\n",
    "    households_csv = '../data/inputs/UKDA-5340-tab/tab/household_eul_2002-2020.tab'\n",
    "    individuals_csv = '../data/inputs/UKDA-5340-tab/tab/individual_eul_2002-2020.tab'\n",
    "    trips_csv ='../data/inputs/UKDA-5340-tab/tab/trip_eul_2002-2020.tab'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T17:13:00.123065Z",
     "start_time": "2020-08-03T17:13:00.093023Z"
    }
   },
   "source": [
    "## Load households data\n",
    "\n",
    "1. Load household data into pandas DataFrame.\n",
    "2. Create some mappings of participation and weighting by household for use later. These are described in http://doc.ukdataservice.ac.uk/doc/5340/mrdoc/pdf/5340_nts_user_guidance_1995-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:02.359648Z",
     "start_time": "2020-11-23T09:59:01.049612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Column adjustments\n",
    "# OutCom_B02ID is missing and is replaced by W1\n",
    "# HRPSEGWorkStat_B01ID is missing\n",
    "\n",
    "hh_in = pd.read_csv(\n",
    "    households_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['HouseholdID', 'SurveyYear', 'PSUID', 'W2', 'W1',\n",
    "       'HHIncome2002_B02ID', 'AddressType_B01ID', 'Ten1_B02ID',\n",
    "       'Landlord_B01ID', 'ResLength_B01ID', 'HHoldCountry_B01ID',\n",
    "       'HHoldGOR_B02ID', 'HHoldNumAdults', 'HHoldNumChildren',\n",
    "       'HHoldNumPeople', 'HHoldStruct_B02ID', 'NumLicHolders',\n",
    "       'HHoldEmploy_B01ID', 'NumVehicles', 'NumBike', 'NumCar', 'NumMCycle',\n",
    "       'NumVanLorry', 'NumCarVan', 'WalkBus_B01ID', 'Getbus_B01ID',\n",
    "       'WalkRail_B01ID', 'WalkRailAlt_B01ID',\n",
    "       'HRPWorkStat_B02ID', 'HRPSEGWorkStat_B01ID', 'HHoldOAClass2011_B03ID',\n",
    "       'Settlement2011EW_B03ID'],\n",
    ")\n",
    "\n",
    "hh_in.HHIncome2002_B02ID = pd.to_numeric(hh_in.HHIncome2002_B02ID, errors='coerce')\n",
    "hh_in.NumLicHolders = pd.to_numeric(hh_in.NumLicHolders, errors='coerce')\n",
    "hh_in.NumVehicles = pd.to_numeric(hh_in.NumVehicles, errors='coerce')\n",
    "hh_in.NumCar = pd.to_numeric(hh_in.NumCar, errors='coerce')\n",
    "hh_in.NumMCycle = pd.to_numeric(hh_in.NumMCycle, errors='coerce')\n",
    "hh_in.NumVanLorry = pd.to_numeric(hh_in.NumVanLorry, errors='coerce')\n",
    "hh_in.NumCarVan = pd.to_numeric(hh_in.NumCarVan, errors='coerce')\n",
    "hh_in.Settlement2011EW_B03ID = pd.to_numeric(hh_in.Settlement2011EW_B03ID, errors='coerce')\n",
    "# hh_in.Settlement2011EW_B04ID = pd.to_numeric(hh_in.Settlement2011EW_B04ID, errors='coerce')\n",
    "\n",
    "\n",
    "hh_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:02.471138Z",
     "start_time": "2020-11-23T09:59:02.362042Z"
    }
   },
   "outputs": [],
   "source": [
    "participation_mapping = dict(zip(hh_in.HouseholdID, hh_in.W1))# OutCom_B02ID\n",
    "weight_mapping = dict(zip(hh_in.HouseholdID, hh_in.W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T10:49:47.862682Z",
     "start_time": "2020-08-04T10:49:47.799479Z"
    }
   },
   "source": [
    "## Load person data\n",
    "\n",
    "Load person attributes data into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:06.030164Z",
     "start_time": "2020-11-23T09:59:02.474033Z"
    }
   },
   "outputs": [],
   "source": [
    "persons_in = pd.read_csv(\n",
    "    individuals_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['SurveyYear', 'IndividualID', 'HouseholdID', 'PSUID', 'VehicleID',\n",
    "       'PersNo', 'Age_B01ID', 'OfPenAge_B01ID', 'Sex_B01ID', 'EdAttn1_B01ID',\n",
    "       'EdAttn2_B01ID', 'EdAttn3_B01ID', 'DrivLic_B02ID', 'CarAccess_B01ID',\n",
    "       'DrivDisable_B01ID', 'WkPlace_B01ID', 'ES2000_B01ID', 'NSSec_B03ID',\n",
    "       'SC_B01ID', 'Stat_B01ID', 'SVise_B01ID', 'EcoStat_B02ID',\n",
    "       'PossHom_B01ID']\n",
    ")\n",
    "persons_in.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trip data\n",
    "\n",
    "1. Load trip data into pandas DataFrame format.\n",
    "2. Apply some preliminary formatting\n",
    "3. Replace headers so that we can use pam read method:\n",
    "\n",
    "\n",
    "- pid - person ID\n",
    "- hid - household ID\n",
    "- seq - trip sequence number\n",
    "- hzone - household zone\n",
    "- ozone - trip origin zone\n",
    "- dzone - trip destination zone\n",
    "- purp - trip purpose\n",
    "- mode - trip mode\n",
    "- tst - trip start time (minutes)\n",
    "- tet - trip end time (minutes)\n",
    "- freq - weighting for representative population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:19.495376Z",
     "start_time": "2020-11-23T09:59:06.038395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Column adjustments\n",
    "# TripDestUA2009_B01ID replaced by TripDestGOR_B02ID\n",
    "# TripOrigUA2009_B01ID replaced by TripOrigGOR_B02ID\n",
    "\n",
    "travel_diaries_in = pd.read_csv(\n",
    "    trips_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['TripID', 'SurveyYear', 'DayID', 'IndividualID', 'HouseholdID', 'PSUID',\n",
    "       'PersNo', 'TravDay', 'JourSeq', 'ShortWalkTrip_B01ID', 'NumStages',\n",
    "       'MainMode_B04ID', 'TripPurpFrom_B01ID',\n",
    "        'TripPurpTo_B01ID', 'TripPurpose_B04ID',\n",
    "       'TripStart', 'TripEnd', 'TripOrigGOR_B02ID', 'TripDestGOR_B02ID'],\n",
    "#     dtype={\"W5\": np.float64,}\n",
    ")\n",
    "\n",
    "travel_diaries_in.TripStart = pd.to_numeric(travel_diaries_in.TripStart, errors='coerce')\n",
    "travel_diaries_in.TripEnd = pd.to_numeric(travel_diaries_in.TripEnd, errors='coerce')\n",
    "\n",
    "travel_diaries_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:19.921306Z",
     "start_time": "2020-11-23T09:59:19.500969Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries_in['participation'] = travel_diaries_in.HouseholdID.map(participation_mapping)\n",
    "travel_diaries_in['hh_weight'] = travel_diaries_in.HouseholdID.map(weight_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.535710Z",
     "start_time": "2020-11-23T09:59:19.924030Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries = travel_diaries_in.loc[travel_diaries_in.participation.isin([1,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.576839Z",
     "start_time": "2020-11-23T09:59:21.538108Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.676906Z",
     "start_time": "2020-11-23T09:59:21.579327Z"
    }
   },
   "outputs": [],
   "source": [
    "# TripOrigUA2009_B01ID was replaced by TripOrigGOR_B02ID\n",
    "# TripDestUA2009_B01ID was replaced by TripDestGOR_B02ID\n",
    "# TripPurpose_B04ID was replaced by \n",
    "\n",
    "travel_diaries.rename(\n",
    "    columns={  # rename data\n",
    "        'JourSeq': 'seq',\n",
    "        'TripOrigGOR_B02ID': 'ozone',\n",
    "        'TripDestGOR_B02ID': 'dzone',\n",
    "        'TripPurpFrom_B01ID': 'oact',\n",
    "        'TripPurpTo_B01ID': 'dact',\n",
    "        'MainMode_B04ID': 'mode',\n",
    "        'TripStart': 'tst',\n",
    "        'TripEnd': 'tet',\n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.720775Z",
     "start_time": "2020-11-23T09:59:21.680364Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.767674Z",
     "start_time": "2020-11-23T09:59:21.729518Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_uniques(df):\n",
    "    for c in df.columns:\n",
    "        print(c)\n",
    "        n = df[c].nunique()\n",
    "        if n < 1000:\n",
    "            print(df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.922539Z",
     "start_time": "2020-11-23T09:59:21.772210Z"
    }
   },
   "outputs": [],
   "source": [
    "check_uniques(travel_diaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Mapping\n",
    "\n",
    "The NTS documentation refers to a 'modified' 2009 Unitary Authorities. The Unmodified 2017 UAs are included below for reference. They 2017 UA names are similar but not the same as the NTS mappings.\n",
    "\n",
    "Found here: https://data.gov.uk/dataset/4e1d5b2c-bb91-42ad-b420-f7fcab638389/counties-and-unitary-authorities-december-2017-full-extent-boundaries-in-uk-wgs84.\n",
    "\n",
    "High resolution spatial boundaries found here: https://data.cambridgeshireinsight.org.uk/dataset/output-areas ans https://martinjc.github.io/UK-GeoJSON/\n",
    "\n",
    "We have built our own geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:21.952200Z",
     "start_time": "2020-11-23T09:59:21.925277Z"
    }
   },
   "outputs": [],
   "source": [
    "area_path = \"../data/inputs/cambridge/cambridge_wards.geojson\"\n",
    "region_path = \"../data/inputs/cambridge/cambridge_boundary.geojson\"\n",
    "# \"./data/Counties_Unitary_Authorities_dec2017_full_extent/NTS_boundaries.geojson\"\n",
    "# \"./data/cambridge/cambridge_bufferred_convex_hull_output_areas.geojson\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:32.378392Z",
     "start_time": "2020-11-23T09:59:21.955206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import areas of interest\n",
    "areas = gp.read_file(area_path)\n",
    "region = gp.read_file(region_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:33.886738Z",
     "start_time": "2020-11-23T09:59:32.383388Z"
    }
   },
   "outputs": [],
   "source": [
    "areas.plot(figsize=(6,6))\n",
    "region.plot(figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean out incomplete plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_clean_travel_diaries = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:59:33.918312Z",
     "start_time": "2020-11-23T09:59:33.888534Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_broken_plans(plan):\n",
    "    if plan.isnull().values.any():\n",
    "        return None\n",
    "    for col in ['ozone', 'dzone']:\n",
    "        if -8 in list(plan[col]):\n",
    "            return None\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_clean_travel_diaries:\n",
    "    # Import clean travel diaries\n",
    "    clean_travel_diaries = pd.read_csv('../data/outputs/UKDA-5340-processed/clean_travel_diaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:57.830263Z",
     "start_time": "2020-11-23T09:59:33.923065Z"
    }
   },
   "outputs": [],
   "source": [
    "if not import_clean_travel_diaries:\n",
    "    clean_travel_diaries = travel_diaries.groupby(\n",
    "        ['IndividualID', 'TravDay']\n",
    "    ).apply(\n",
    "        remove_broken_plans\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    print('Exporting clean travel diaries')\n",
    "    # Export clean travel diaries\n",
    "#     clean_travel_diaries.to_csv('./data/UKDA-5340-processed/clean_travel_diaries.csv')#crs=\"EPSG:27700\", to_crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:57.970972Z",
     "start_time": "2020-11-23T10:04:57.833810Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:58.083788Z",
     "start_time": "2020-11-23T10:04:57.992403Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(travel_diaries))\n",
    "print(len(clean_travel_diaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Mappings and apply to common fields\n",
    "\n",
    "We simplify key trip variables such as mode and activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:58.172340Z",
     "start_time": "2020-11-23T10:04:58.103360Z"
    }
   },
   "outputs": [],
   "source": [
    "def string_to_dict(string):\n",
    "    \"\"\"used to build dicts from NTS rtf format dictionaries (cut and paste from the NTS documentation)\"\"\"\n",
    "    mapping = {}\n",
    "    for line in string.split(\"\\n\"):\n",
    "        _, v, l = line.split(\"\\t\")\n",
    "        v = v.split(\" = \")[1]\n",
    "        l = l.split(\" = \")[1]\n",
    "        mapping[float(v)] = str(l)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:04:58.466019Z",
     "start_time": "2020-11-23T10:04:58.185590Z"
    }
   },
   "outputs": [],
   "source": [
    "mode_mapping = {\n",
    "    1: 'walk',\n",
    "     2: 'bike',\n",
    "     3: 'car',  #'Car/van driver'\n",
    "     4: 'car',  #'Car/van driver'\n",
    "     5: 'car',  #'Motorcycle',\n",
    "     6: 'car',  #'Other private transport',\n",
    "     7: 'bus', #Bus in London',\n",
    "     8: 'bus', #'Other local bus',\n",
    "     9: 'bus', #'Non-local bus',\n",
    "     10: 'rail', #'London Underground',\n",
    "     11: 'rail', #'Surface Rail',\n",
    "     12: 'car',  #'Taxi/minicab',\n",
    "     13: 'car', #'Other public transport',\n",
    "     -10: 'DEAD',\n",
    "     -8: 'NA'\n",
    "}\n",
    "\n",
    "purp_mapping = {\n",
    "    1: 'work',\n",
    "     2: 'work',  #'In course of work',\n",
    "     3: 'education',\n",
    "     4: 'shop',  #'Food shopping',\n",
    "     5: 'shop',  #'Non food shopping',\n",
    "     6: 'medical', #'Personal business medical',\n",
    "     7: 'other',  #'Personal business eat/drink',\n",
    "     8: 'other',  #'Personal business other',\n",
    "     9: 'other',  #'Eat/drink with friends',\n",
    "     10: 'visit',  #'Visit friends',\n",
    "     11: 'other',  #'Other social',\n",
    "     12: 'other',  #'Entertain/ public activity',\n",
    "     13: 'other',  #'Sport: participate',\n",
    "     14: 'home',  #'Holiday: base',\n",
    "     15: 'other',  #'Day trip/just walk',\n",
    "     16: 'other',  #'Other non-escort',\n",
    "     17: 'escort_home',  #'Escort home',\n",
    "     18: 'escort_work',  #'Escort work',\n",
    "     19: 'escort_work',  #'Escort in course of work',\n",
    "     20: 'escort_education',  #'Escort education',\n",
    "     21: 'escort_shop',  #'Escort shopping/personal business',\n",
    "     22: 'escort_other',  #'Other escort',\n",
    "     23: 'home',  #'Home',\n",
    "     -10: 'DEAD',\n",
    "     -8: 'NA'\n",
    "}\n",
    "\n",
    "clean_travel_diaries['mode'] = clean_travel_diaries['mode'].map(mode_mapping)\n",
    "clean_travel_diaries['oact'] = clean_travel_diaries['oact'].map(purp_mapping)\n",
    "clean_travel_diaries['dact'] = clean_travel_diaries['dact'].map(purp_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweight and Split Days\n",
    "\n",
    "In order to get the most from our small sample we treat individual diary days as new persons. In order to maintain the original household weighting we reduce this accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_trips = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_trips:\n",
    "    # Import clean travel diaries\n",
    "    trips = pd.read_csv('../data/outputs/UKDA-5340-processed/trips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:07:34.500482Z",
     "start_time": "2020-11-23T10:04:58.468222Z"
    }
   },
   "outputs": [],
   "source": [
    "# reweight and split ids for unique days\n",
    "\n",
    "def reweight(group):\n",
    "    \"\"\"\n",
    "    Reweight based on multiple diary days, ie if an agent has two diary days, we will treat these as\n",
    "    two unique agents, so we half the original weighting\n",
    "    \"\"\"\n",
    "    group['freq'] = group.hh_weight / group.DayID.nunique()\n",
    "    return group\n",
    "    \n",
    "if not import_trips:\n",
    "\n",
    "    # Make sure household weights are floats\n",
    "    clean_travel_diaries.hh_weight = clean_travel_diaries.hh_weight.apply(float)\n",
    "\n",
    "#     trips = clean_travel_diaries.groupby('IndividualID').apply(reweight)\n",
    "    trips.freq = clean_travel_diaries.groupby('IndividualID').apply(reweight).freq\n",
    "    trips['pid'] = [f\"{p}_{d}\" for p, d in zip(trips.IndividualID, trips.TravDay)]\n",
    "    trips['hid'] = [f\"{h}_{d}\" for h, d in zip(trips.HouseholdID, trips.TravDay)]\n",
    "    \n",
    "    trips = trips.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "    trips['hzone'] = pd.merge(trips,hh_in[['HouseholdID','PSUID','HHoldGOR_B02ID']],on=['HouseholdID','PSUID'],how='left')['HHoldGOR_B02ID']\n",
    "    # Export trips\n",
    "#     trips.to_csv('./data/UKDA-5340-processed/trips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:07:34.763851Z",
     "start_time": "2020-11-23T10:07:34.712210Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_days(\n",
    "    trips,\n",
    "    target,\n",
    "    trips_on='Diary_number',\n",
    "    target_on='Diary_number',\n",
    "    new_id='pid',\n",
    "    trim=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Expand target df based on mapping between trips target_on and new_id.\n",
    "    This is so slow. Fix\n",
    "    Set index to new_id.\n",
    "    \"\"\"\n",
    "    print(\"Building mapping.\")\n",
    "    mapping = {}\n",
    "    for i, person in trips.groupby(target_on):\n",
    "        mapping[i] = list(set(person[new_id]))\n",
    "    n = len(mapping)\n",
    "    \n",
    "    if trim:\n",
    "        print(\"Trimming target.\")\n",
    "        selection = set(trips[trips_on])\n",
    "        target = target.loc[target[target_on].isin(selection)]\n",
    "    \n",
    "    expanded = pd.DataFrame()\n",
    "    for p, (i, ids) in enumerate(mapping.items()):\n",
    "        if not p % 10:\n",
    "            print(f\"Building expanded data {p}/{n}\", end='\\r', flush=True)\n",
    "        for idx in ids:\n",
    "            split = target.loc[target[target_on] == i]\n",
    "            split[new_id] = idx\n",
    "            expanded = expanded.append(split)\n",
    "    expanded.set_index(new_id, inplace=True)\n",
    "    print(f\"Done\")\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_hhs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:58:31.895656Z",
     "start_time": "2020-11-23T10:07:34.767754Z"
    }
   },
   "outputs": [],
   "source": [
    "if not import_hhs:\n",
    "    hhs = expand_days(\n",
    "        trips,\n",
    "        hh_in,\n",
    "        trips_on='HouseholdID',\n",
    "        target_on='HouseholdID',\n",
    "        new_id='hid'\n",
    "    )\n",
    "    hhs = hhs.rename(columns={\"HHoldGOR_B02ID\":\"hzone\"})\n",
    "#     hhs.to_csv('./data/UKDA-5340-processed/hhs.csv')\n",
    "else:\n",
    "    hhs = pd.read_csv('../data/outputs/UKDA-5340-processed/hhs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_people = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:07.759760Z",
     "start_time": "2020-11-23T10:58:31.993216Z"
    }
   },
   "outputs": [],
   "source": [
    "if not import_people:\n",
    "    people = expand_days(\n",
    "        trips,\n",
    "        persons_in,\n",
    "        trips_on='IndividualID',\n",
    "        target_on='IndividualID',\n",
    "        new_id='pid'\n",
    "    )\n",
    "#     people.to_csv('./data/UKDA-5340-processed/people.csv')\n",
    "else:\n",
    "    people = pd.read_csv('../data/outputs/UKDA-5340-processed/people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:07.823487Z",
     "start_time": "2020-11-23T12:10:07.762160Z"
    }
   },
   "outputs": [],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset trips, people and households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_cambridge_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all household, trip and people ids\n",
    "all_hh_ids_len = hhs.HouseholdID.nunique()\n",
    "all_trip_ids_len = trips.TripID.nunique()\n",
    "all_people_ids_len = people.shape[0]#IndividualID.nunique()\n",
    "\n",
    "\n",
    "# Get all household ids that satisfy\n",
    "# Household (HHoldGOR_B02ID or hzone) must be in East England (6) OR \n",
    "cambridge_domestic_hh_ids = set(hhs[hhs.hzone.isin([6])].HouseholdID.unique().tolist())\n",
    "# Trip origin AND destination must be the same and in the East of England (6)\n",
    "cambridge_domestic_trip_hh_ids = set(trips[(trips.ozone == trips.dzone) & (trips.ozone.isin([6]))].HouseholdID.unique().tolist())\n",
    "# AND then take\n",
    "# Get all households before January 2020 (pandemic)\n",
    "cambridge_2002_2019_hh_ids = set(hhs[hhs.HouseholdID.apply(lambda x: not str(x).startswith('2020'))].HouseholdID.unique().tolist())\n",
    "\n",
    "# Take union of first two conditions and then intersection with last condition\n",
    "cambridge_hh_ids = list((cambridge_domestic_hh_ids.union(cambridge_domestic_trip_hh_ids)).intersection(cambridge_2002_2019_hh_ids))\n",
    "\n",
    "# Subset household, trips and people based on household ids\n",
    "cambridge_hhs = hhs[hhs.HouseholdID.isin(cambridge_hh_ids)].reset_index()#.set_index('hid')\n",
    "cambridge_trips = trips[trips.HouseholdID.isin(cambridge_hh_ids)].reset_index(drop=True)\n",
    "cambridge_people = people[people.HouseholdID.isin(cambridge_hh_ids)].reset_index()#.set_index('pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cambridge_trip_ids_len = len(cambridge_trips.TripID.unique())\n",
    "cambridge_people_ids_len = cambridge_people.shape[0]\n",
    "\n",
    "\n",
    "print(f'{len(cambridge_hh_ids)} households maintained ({int(100*len(cambridge_hh_ids)/all_hh_ids_len)} % of total households available) ')\n",
    "print(f'{cambridge_trip_ids_len} trips maintained ({int(100*cambridge_trip_ids_len/all_trip_ids_len)} % of total trips available) ')\n",
    "print(f'{cambridge_people_ids_len} people maintained ({int(100*cambridge_people_ids_len/all_people_ids_len)} % of total people available) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "cambridge_hhs.drop(columns = {\"index\",\"Unnamed: 0\"},inplace=True)\n",
    "cambridge_trips.drop(columns = {\"Unnamed: 0\",\"Unnamed: 0.1\",\"Unnamed: 0.1.1\"},inplace=True)\n",
    "cambridge_people.drop(columns = {\"index\"},inplace=True)\n",
    "# Reset index\n",
    "# cambridge_hhs = cambridge_hhs.reset_index()\n",
    "# cambridge_people = cambridge_people.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cambridge NTS tables\n",
    "if export_cambridge_data:\n",
    "    cambridge_hhs.to_csv('../data/outputs/UKDA-5340-processed/cambridge_hhs.csv')\n",
    "    cambridge_people.to_csv('../data/outputs/UKDA-5340-processed/cambridge_people.csv')\n",
    "    cambridge_trips.to_csv('../data/outputs/UKDA-5340-processed/cambridge_trips.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into PAM\n",
    "\n",
    "We load the pandas formatted data into Pam using the `pam.read.load_travel_diary_from_to` read method. We do some very preliminary validation of plans and assurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:07.959695Z",
     "start_time": "2020-11-23T12:10:07.835753Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam import write\n",
    "from pam import read\n",
    "from pam.plot.stats import plot_activity_times, plot_leg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:08.008552Z",
     "start_time": "2020-11-23T12:10:07.963560Z"
    }
   },
   "outputs": [],
   "source": [
    "cambridge_trips.tst = cambridge_trips.tst.astype(int)\n",
    "cambridge_trips.tet = cambridge_trips.tet.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:17.801118Z",
     "start_time": "2020-11-23T12:10:08.014383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population = read.load_travel_diary(\n",
    "    trips=cambridge_trips,\n",
    "    persons_attributes=cambridge_people,\n",
    "    hhs_attributes=cambridge_hhs,\n",
    "    trip_freq_as_person_freq=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:17.999106Z",
     "start_time": "2020-11-23T12:10:17.804047Z"
    }
   },
   "outputs": [],
   "source": [
    "population.fix_plans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.064982Z",
     "start_time": "2020-11-23T12:10:18.001901Z"
    }
   },
   "outputs": [],
   "source": [
    "# this should be replaced with a more direct method\n",
    "for hh in tqdm(population.households.values()):\n",
    "    for p in hh.people.values():\n",
    "        p.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.106291Z",
     "start_time": "2020-11-23T12:10:18.067138Z"
    }
   },
   "outputs": [],
   "source": [
    "population.size  # this also accounts for the weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.169445Z",
     "start_time": "2020-11-23T12:10:18.119285Z"
    }
   },
   "outputs": [],
   "source": [
    "population.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.219002Z",
     "start_time": "2020-11-23T12:10:18.176008Z"
    }
   },
   "outputs": [],
   "source": [
    "population.activity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:18.270601Z",
     "start_time": "2020-11-23T12:10:18.222929Z"
    }
   },
   "outputs": [],
   "source": [
    "population.mode_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:20.473655Z",
     "start_time": "2020-11-23T12:10:18.277070Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_activity_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:21.682828Z",
     "start_time": "2020-11-23T12:10:20.483760Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_leg_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:21.728035Z",
     "start_time": "2020-11-23T12:10:21.693486Z"
    }
   },
   "outputs": [],
   "source": [
    "# night shift @ 2016008863_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T10:58:14.515344Z",
     "start_time": "2020-11-25T10:58:14.237045Z"
    }
   },
   "outputs": [],
   "source": [
    "hh = population.random_household()\n",
    "hh.print()\n",
    "hh.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T10:58:58.742366Z",
     "start_time": "2020-11-25T10:58:58.698118Z"
    }
   },
   "outputs": [],
   "source": [
    "population.activity_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Population\n",
    "\n",
    "We sample a very small population based on the given NTS household weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:26.313000Z",
     "start_time": "2020-11-23T12:10:22.120024Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam.core import Population\n",
    "from pam.samplers.basic import freq_sample\n",
    "# from copy import deepcopy\n",
    "\n",
    "population_sample = Population()\n",
    "    \n",
    "for hid, household in tqdm(population.households.items()):\n",
    "    av_hh_weight = household.freq  # this is currently the av of person freq in the hh\n",
    "    freq = freq_sample(av_hh_weight, 10)\n",
    "\n",
    "    for idx in range(freq):\n",
    "        hh = deepcopy(household)\n",
    "        hh.hid = f\"{hh.hid}_{idx}\"\n",
    "        hh.people = {}\n",
    "        for pid, person in household.people.items():\n",
    "            p = deepcopy(person)\n",
    "            p.pid = f\"{pid}_{idx}\"\n",
    "            hh.add(p)\n",
    "        population_sample.add(hh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:10:26.354636Z",
     "start_time": "2020-11-23T12:10:26.315503Z"
    }
   },
   "outputs": [],
   "source": [
    "population_sample.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facility SamplingÂ¶ \n",
    "\n",
    "The facilities input is prepared using a separate project called OSM-Facility Sampler (OSMFS). This project woulbe be better names the OSM Facility *Extractor*. We use it to extract viable activity locations for each activity type for each zone. This project is not currently open source, but is described below:\n",
    "\n",
    "OSMFS joins osm data with the geographies of an area to create a mapping between zones, acts and facility locations (points). This is output as a geojson:\n",
    "\n",
    "{\"type\": \"FeatureCollection\", \"features\": [{\"id\": \"0\", \"type\": \"Feature\", \"properties\": {\"activity\": \"other\"}, \"geometry\": {\"type\": \"Point\", \"coordinates\": [-4.5235751, 54.1698685]}},\n",
    "\n",
    "todo: the current methodology does not support shared facilities, ie facilities with more than one activity (schools are places of education and work for example).\n",
    "\n",
    "todo: the above json has to be rejoined with the geography to create a spatial sampler. This is a duplicated operation which could be included in the Bench output, eg:\n",
    "\n",
    "zone_id: activity: (id, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T19:41:13.503578Z",
     "start_time": "2020-11-23T19:41:13.475775Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam.samplers import facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_facilities(path, from_crs=\"EPSG:4326\", to_crs=\"EPSG:27700\"):\n",
    "    \n",
    "    facilities = gp.read_file(path)\n",
    "    facilities = facilities.rename(columns={\"activities\":\"activity\"})\n",
    "    # Reproject if necessary\n",
    "    if from_crs != to_crs: \n",
    "        print('Reprojecting facilities')\n",
    "        facilities.crs = from_crs\n",
    "        facilities.to_crs(to_crs, inplace=True)\n",
    "    facilities.crs = to_crs\n",
    "    return facilities\n",
    "\n",
    "def load_zones(zones_path, from_crs=\"EPSG:27700\", to_crs=\"EPSG:27700\"):\n",
    "    \n",
    "    zones = gp.read_file(zones_path)\n",
    "    zones.set_index('ward_code', inplace=True)\n",
    "    if not from_crs == to_crs:\n",
    "        zones.crs = from_crs\n",
    "        zones.to_crs(to_crs, inplace=True)\n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T19:41:13.882483Z",
     "start_time": "2020-11-23T19:41:13.854988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import wards and facilities in Cambridge\n",
    "cambridge_facilities_path = './data/cambridge/cambridge_facilities_spread_filtered.geojson'\n",
    "# './data/cambridge/cambridge_facilities_filtered.geojson'\n",
    "cambridge_facilities = load_facilities(cambridge_facilities_path, from_crs=\"EPSG:27700\")\n",
    "wards = load_zones(area_path, from_crs=\"EPSG:27700\")\n",
    "# wards = wards.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise wards and facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (10,7))\n",
    "wards.boundary.plot(ax = ax)\n",
    "cambridge_facilities.plot(ax=ax, markersize=2, color='purple')\n",
    "# for zone, centroid in zip(zones.index, zones.centroid):\n",
    "#     ax.annotate(zone, xy = (centroid.x, centroid.y), size = 15)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cambridge_facilities.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cambridge_facilities.activity.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch facility location sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sampler_distance(population, sampler, n_iterations = 200, complex_sampler = False,\n",
    "                         title=None):\n",
    "    distance_commute = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        if complex_sampler:\n",
    "            population.sample_locs_complex(sampler)\n",
    "        else:\n",
    "            population.sample_locs(sampler)\n",
    "        distance_commute.append(write.write_benchmarks(population)['euclidean_distance'][0])\n",
    "\n",
    "    pd.Series(distance_commute).hist(bins = 20)\n",
    "    if title!=None:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('distance')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gp.sjoin(cambridge_facilities, wards, how='inner', op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_on = None\n",
    "activity_areas_dict = {x:{} for x in temp['index_right'].unique()}\n",
    "for (zone, act), facility_data in temp.groupby(['index_right', 'activity']):\n",
    "    activity_areas_dict[zone][act] = facility_data\n",
    "sampler_dict = {}\n",
    "activities = list(set(cambridge_facilities.activity))\n",
    "for zone in set(temp.index_right):\n",
    "    sampler_dict[zone] = {}\n",
    "    zone_facs = activity_areas_dict.get(zone, {})\n",
    "\n",
    "    for act in activities:\n",
    "        facs = zone_facs.get(act, None)\n",
    "        if facs is not None:\n",
    "            points = [(i, g) for i, g in facs.geometry.items()]\n",
    "            if weight_on is not None:\n",
    "                # weighted sampler\n",
    "                weights = facs[weight_on]\n",
    "                transit_distance = facs['transit'] if max_walk is not None else None\n",
    "                sampler_dict[zone][act] = 1\n",
    "            else:\n",
    "                # simple sampler\n",
    "                sampler_dict[zone][act] = 1\n",
    "        else:\n",
    "            sampler_dict[zone][act] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_sampler = facility.FacilitySampler(\n",
    "    facilities=cambridge_facilities,\n",
    "    zones=wards,\n",
    "    build_xml=True,\n",
    "    fail=False,\n",
    "    random_default=False\n",
    ")\n",
    "\n",
    "facility_sampler.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = population_sample.random_person()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.home_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.home.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sample.sample_locs(facility_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = population_sample.random_person()\n",
    "person.plot()\n",
    "person.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler_distance(population_sample, facility_sampler, complex_sampler=False,\n",
    "                     title = 'Commute distance, simple sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ward not containing any facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select random facility index\n",
    "# random_index = np.random.randint(0,facilities.shape[0],1)\n",
    "# # Get facility for that index\n",
    "# fac = gp.GeoDataFrame(cambridge_facilities.iloc[random_index,:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,1, figsize = (15,15))\n",
    "# wards[wards.index == 'E05002816'].boundary.plot(ax = ax)\n",
    "# facilities.plot(ax=ax, markersize=100, color='red')\n",
    "# for zone, centroid in zip(wards.index, wards.centroid):\n",
    "#     ax.annotate(zone, xy = (centroid.x, centroid.y), size = 10)\n",
    "# ax.axis('off')\n",
    "# plt.show()\n",
    "# print('Ward code indentified',fac.ward_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampler\n",
    "\n",
    "Failing a facility sampler - we can use random sampling instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pam.samplers.spatial import RandomPointSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zones = load_zones(area_path, from_crs=\"EPSG:4326\")\n",
    "zones = deepcopy(areas)\n",
    "sampler = RandomPointSampler(geoms=zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.sample(113, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sample.sample_locs(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:51:18.843528Z",
     "start_time": "2020-11-24T18:51:18.545189Z"
    }
   },
   "outputs": [],
   "source": [
    "person = population_sample.random_person()\n",
    "person.plot()\n",
    "person.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:48:33.844210Z",
     "start_time": "2020-11-11T11:48:33.815280Z"
    }
   },
   "source": [
    "## Write to Disk\n",
    "\n",
    "1. write MATSim formats to disk (plans and attributes)\n",
    "2. write csv and geojson summaries to disk\n",
    "3. write MATSim formatted facilities to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pam.write as write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:52:43.705393Z",
     "start_time": "2020-11-24T18:52:28.634010Z"
    }
   },
   "outputs": [],
   "source": [
    "comment = 'NTS cambridge prelim 03122020 epsg27700' \n",
    "#'NTS london prelim 24nov2020 epsg27700'\n",
    "\n",
    "write.write_matsim(\n",
    "        population_sample,\n",
    "        plans_path=os.path.join(out_dir, 'plans.xml'),\n",
    "        attributes_path=os.path.join(out_dir, 'attributes.xml'),\n",
    "        comment=comment\n",
    "    )\n",
    "population_sample.to_csv(out_dir, crs=\"EPSG:27700\", to_crs=\"EPSG:4326\")\n",
    "# facility_sampler.write_facilities_xml(os.path.join(out_dir, 'facilities.xml'), comment=comment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
